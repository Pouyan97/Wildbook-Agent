{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import loadProperties, loadWEKA\n",
    "\n",
    "props = loadProperties('submitActionClass.properties')\n",
    "(data, attr) = loadWEKA('youTubeLocationIDWeka.csv', limit=0)\n",
    "data2 = data[11000:11800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting spacy's pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "locations = [x for x in props] # Array of known locations from .properties\n",
    "location_patterns = list(nlp.pipe(locations))\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "matcher.add(\"LOCATION\", None, *location_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "def location_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'LOCATION'\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = [Span(doc, start, end, label=\"LOCATION\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component\n",
    "nlp.add_pipe(location_component, before='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses spacy to look up for location in a strings. Returns Array of matches\n",
    "def nlpLocation(string):\n",
    "    spacy_mathc = []\n",
    "    for ent in nlp(string).ents:\n",
    "        if ent.label_ == \"GPE\" or ent.label_ == \"LOCATION\":\n",
    "            spacy_mathc.append(ent.text.lower())\n",
    "    return spacy_mathc\n",
    "\n",
    "def matchLocationV3(item_original):\n",
    "    item = item_original.copy()\n",
    "    locations = []\n",
    "\n",
    "    # Filtering only items with title/descr/tags\n",
    "    if item[2] or item[3] or item[4]:\n",
    "        # Processing with spacy\n",
    "        locations += nlpLocation(item[2].replace(\"'\",\"\")) # Title\n",
    "        locations += nlpLocation(item[3].replace(\"'\",\"\")) # Tags\n",
    "        locations += nlpLocation(item[4].replace(\"'\",\"\")) # Descr\n",
    "      \n",
    "    item.append(locations)\n",
    "    return item\n",
    "\n",
    "# matchLocationV3(data2[405])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicateLocation(item_original):\n",
    "    item = item_original.copy()\n",
    "    item[7] = list(set(item[7]))\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printItem(item_original):\n",
    "    print(item_original)\n",
    "    print()\n",
    "    return item_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countStats(data):\n",
    "    total = len(data)\n",
    "    hasDataToAnalyze = 0\n",
    "    identified = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if item[2] or item[3] or item[4]:\n",
    "            hasDataToAnalyze += 1\n",
    "        if len(item[7]) > 0:\n",
    "            identified += 1\n",
    "            \n",
    "    print(total, \"items were processed in total.\")\n",
    "    print(hasDataToAnalyze, \"of them had title,description or tags to analyze.\")\n",
    "    print(identified, \"out of\", hasDataToAnalyze, \"were matched with potential location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 items were processed in total.\n",
      "192 of them had title,description or tags to analyze.\n",
      "33 out of 192 were matched with potential location\n"
     ]
    }
   ],
   "source": [
    "from customPipeline import Pipe\n",
    "pl = Pipe()\n",
    "\n",
    "pl.addPipe(matchLocationV3)\n",
    "pl.addPipe(removeDuplicateLocation)\n",
    "# pl.addPipe(printItem)\n",
    "\n",
    "result = pl(data2)\n",
    "\n",
    "countStats(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
